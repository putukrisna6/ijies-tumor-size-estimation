{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2dfb00df-3b4e-4854-b77f-bf00371d2b93",
      "metadata": {
        "id": "2dfb00df-3b4e-4854-b77f-bf00371d2b93"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813dcec8",
      "metadata": {
        "id": "813dcec8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path(\"notebook_utils.py\").exists():\n",
        "    r = requests.get(\n",
        "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
        "    )\n",
        "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
        "\n",
        "if not Path(\"cmd_helper.py\").exists():\n",
        "    r = requests.get(\n",
        "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\",\n",
        "    )\n",
        "    open(\"cmd_helper.py\", \"w\").write(r.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243c8e5f",
      "metadata": {
        "id": "243c8e5f"
      },
      "outputs": [],
      "source": [
        "from cmd_helper import clone_repo\n",
        "\n",
        "clone_repo(\"https://huggingface.co/spaces/depth-anything/Depth-Anything-V2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8633459d-546c-4bca-a0fd-e8fe81dbed27",
      "metadata": {
        "id": "8633459d-546c-4bca-a0fd-e8fe81dbed27"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "%pip install -q \"openvino>=2024.2.0\" \"datasets>=2.14.6\" \"nncf>=2.11.0\" \"tqdm\" \"matplotlib>=3.4\"\n",
        "%pip install -q \"typing-extensions>=4.9.0\" eval-type-backport \"gradio>=4.19\" gradio_imageslider\n",
        "%pip install -q torch torchvision \"opencv-python\" huggingface_hub --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "if platform.system() == \"Darwin\":\n",
        "    %pip install -q \"numpy<2.0.0\"\n",
        "if platform.python_version_tuple()[1] in [\"8\", \"9\"]:\n",
        "    %pip install -q \"gradio-imageslider<=0.0.17\" \"typing-extensions>=4.9.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f1c16c-0d0f-49aa-bc45-c1fb7a59f0bf",
      "metadata": {
        "id": "84f1c16c-0d0f-49aa-bc45-c1fb7a59f0bf"
      },
      "source": [
        "## Load and run PyTorch model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c81ccd-0fda-4e01-b81b-0bfa2cff4ce5",
      "metadata": {
        "id": "34c81ccd-0fda-4e01-b81b-0bfa2cff4ce5"
      },
      "source": [
        "To be able run PyTorch model on CPU, we should disable xformers attention optimizations first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcf2907-f5f4-4f21-9a3e-6cbe5bbd931a",
      "metadata": {
        "id": "afcf2907-f5f4-4f21-9a3e-6cbe5bbd931a"
      },
      "outputs": [],
      "source": [
        "attention_file_path = Path(\"./Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py\")\n",
        "orig_attention_path = attention_file_path.parent / (\"orig_\" + attention_file_path.name)\n",
        "\n",
        "if not orig_attention_path.exists():\n",
        "    attention_file_path.rename(orig_attention_path)\n",
        "\n",
        "    with orig_attention_path.open(\"r\") as f:\n",
        "        data = f.read()\n",
        "        data = data.replace(\"XFORMERS_AVAILABLE = True\", \"XFORMERS_AVAILABLE = False\")\n",
        "        with attention_file_path.open(\"w\") as out_f:\n",
        "            out_f.write(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c2822f-c907-4c54-9c4c-a60bc8182e17",
      "metadata": {
        "id": "60c2822f-c907-4c54-9c4c-a60bc8182e17"
      },
      "source": [
        "### Prepare input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f834c75f-3e5c-475e-8c1b-9d5d53e7b342",
      "metadata": {
        "id": "f834c75f-3e5c-475e-8c1b-9d5d53e7b342"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from notebook_utils import download_file, device_widget, quantization_widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bae63e8",
      "metadata": {
        "id": "2bae63e8"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "encoder = \"vitb\"\n",
        "model_type = \"Base\"\n",
        "model_id = f\"depth_anything_v2_{encoder}\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=f\"depth-anything/Depth-Anything-V2-{model_type}\", filename=f\"{model_id}.pth\", repo_type=\"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ddac43a-fd73-46a8-a6b5-669dba7907d5",
      "metadata": {
        "id": "3ddac43a-fd73-46a8-a6b5-669dba7907d5"
      },
      "source": [
        "Preprocessed image passed to model forward and model returns depth map in format `B` x `H` x `W`, where `B` is input batch size, `H` is preprocessed image height, `W` is preprocessed image width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5020c0fc",
      "metadata": {
        "id": "5020c0fc"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "model = DepthAnythingV2(encoder=encoder, features=128, out_channels=[96, 192, 384, 768])\n",
        "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "raw_img = cv2.imread(\"/content/input.jpg\")\n",
        "image, (h, w) = model.image2tensor(raw_img)\n",
        "image = image.to(\"cpu\").to(torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    depth = model.forward(image)\n",
        "\n",
        "depth = F.interpolate(depth[:, None], (h, w), mode=\"bilinear\", align_corners=True)[0, 0]\n",
        "\n",
        "output = depth.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2c7f91",
      "metadata": {
        "id": "1a2c7f91"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def get_depth_map(output, w, h):\n",
        "    depth = cv2.resize(output, (w, h))\n",
        "\n",
        "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
        "    depth = depth.astype(np.uint8)\n",
        "\n",
        "    depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
        "\n",
        "    return depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45bc2ae5",
      "metadata": {
        "id": "45bc2ae5"
      },
      "outputs": [],
      "source": [
        "h, w = raw_img.shape[:-1]\n",
        "res_depth = get_depth_map(output, w, h)\n",
        "plt.imshow(res_depth[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imsave('/content/res_depth.png', res_depth[:, :, ::-1])\n"
      ],
      "metadata": {
        "id": "rh2zErBtnL0N"
      },
      "id": "rh2zErBtnL0N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "openvino_notebooks": {
      "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/depth-anything/depth-anything.gif?raw=true",
      "tags": {
        "categories": [
          "Model Demos",
          "AI Trends"
        ],
        "libraries": [],
        "other": [],
        "tasks": [
          "Depth Estimation"
        ]
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}